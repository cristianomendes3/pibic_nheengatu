import pandas as pd
import json
import re
from transformers import AutoTokenizer
from normalizer import clean_text_nheengatu

# Configura√ß√µes de Arquivo
ARQUIVO_ENTRADA_BRUTO = "100palavras_nheengatu_completo.xlsx"
ARQUIVO_SAIDA_JSON = "dataset_nheengatu_expandido.json"
ARQUIVO_SAIDA_CSV = "dataset_nheengatu_expandido.csv" # √ötil para inspe√ß√£o visual no Excel
MODELO_NOME = "dominguesm/canarim-bert-nheengatu"

def carregar_dados_brutos():
  """Carrega a planilha original com suporte a m√∫ltiplas abas se necess√°rio."""
  try:
    # engine='openpyxl' √© essencial para arquivos .xlsx
    df = pd.read_excel(ARQUIVO_ENTRADA_BRUTO, engine='openpyxl')
    
    # Normaliza√ß√£o dos cabe√ßalhos (remove espa√ßos e converte para T√≠tulo)
    df.columns = [c.strip().title() for c in df.columns]

    # Valida√ß√£o b√°sica
    if 'Palavra' not in df.columns or 'Significado' not in df.columns:
      raise ValueError("As colunas 'Palavra' e 'Significado' s√£o obrigat√≥rias.")

    print(f"Planilha '{ARQUIVO_ENTRADA_BRUTO}' carregada com sucesso!")
    return df

  except Exception as e:
    print(f"Erro ao carregar a planilha '{ARQUIVO_ENTRADA_BRUTO}': {e}")
    return None

def expandir_linha(row):
  """
  Recebe uma linha do DataFrame e retorna uma lista de dicion√°rios expandidos.
  Realiza o 'Data Augmentation' via produto cartesiano.
  """
  raw_words = str(row['Palavra'])
  raw_meanings = str(row['Significado'])

  # Regex para separar m√∫ltiplos itens
  # Separa por v√≠rgula (,), ponto e v√≠rgula (;), barra (/) ou quebra de linha (\n)
  # O \s* remove espa√ßos extras ao redor dos separadores.

  split_patterns = r'[;,/\n]\s*|,\s+'

  lista_palavras = re.split(split_patterns, raw_words)
  lista_significados = re.split(split_patterns, raw_meanings)

  # Limpeza b√°sica (strip) e remo√ß√£o de itens vazios
  lista_palavras = [w.strip() for w in lista_palavras if w.strip()]
  lista_significados = [m.strip() for m in lista_significados if m.strip()]

  pares_expandidos = []

  # Produto Cartesiano: Cada variante x Cada significado
  for palavra in lista_palavras:
    for significado in lista_significados:
      pares_expandidos.append({
          "palavra_original": palavra, 
          "significado_original": significado,
          "origem_linha": row.name + 2 # +2 para ajustar ao √≠ndice do Excel(Header=1, Index=0)
            })

  return pares_expandidos

def processar_augmentacao(df, tokenizer):
  dataset_final = []
  stats = {"original_rows": len(df), "expanded_rows": 0, "unk_tokens": 0}

  print(f"--- Iniciando Augmenta√ß√£o de Dados ---")

  for index, row in df.iterrows():
    # 1. Expans√£o (Augmentation)
    pares = expandir_linha(row)

    for item in pares:
      # 2. Normaliza√ß√£o
      # A palavra √© limpa (lowercase, NFC, remove pontua√ß√£o exceto glotal)
      palavra_norm = clean_text_nheengatu(item['palavra_original'])

      # O significado em portugu√™s tamb√©m passa por limpeza leve (opcional)
      significado_clean = item['significado_original'].strip()

      # 3. Tokeniza√ß√£o e Valida√ß√£o
      tokens = tokenizer.tokenize(palavra_norm)
      ids = tokenizer.convert_tokens_to_ids(tokens)

      # Verifica se o token [UNK] (ID 100 ou similar) apareceu
      tem_unk = tokenizer.unk_token in tokens
      if tem_unk:
        stats["unk_tokens"] += 1
        status = "ALERTA"
      else:
        status = "OK"

      # Monta o objeto final
      entry = {
          "nheengatu_text": palavra_norm,
          "portuguese_text": significado_clean,
          "tokens": tokens,
          "input_ids": ids,
          "tem_unk": tem_unk,
          "metadata": {
              "raw_nheengatu": item['palavra_original'],
              "source_line": item['origem_linha']
          }
      }
      dataset_final.append(entry)
      stats["expanded_rows"] += 1

      # Log visual r√°pido no terminal
      print(f"[{status}] {palavra_norm:<15} -> {str(tokens)}")

  return dataset_final, stats

def main():
    # Carregar Tokenizer
    print(f"‚è≥ Carregando Tokenizer: {MODELO_NOME}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODELO_NOME)
    except Exception as e:
        print(f"‚ùå Erro ao baixar modelo: {e}")
        return

    # Ingest√£o
    df = carregar_dados_brutos()
    if df is None: return

    # Processamento
    dataset, estatisticas = processar_augmentacao(df, tokenizer)

    # Salvamento JSON (Para a m√°quina/treinamento)
    with open(ARQUIVO_SAIDA_JSON, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)

    # Salvamento CSV (Para humanos conferirem se a separa√ß√£o funcionou)
    df_export = pd.DataFrame(dataset)
    # Removemos colunas complexas para o CSV ficar leg√≠vel no Excel
    df_export_simple = df_export.drop(columns=['tokens', 'input_ids', 'metadata'])
    df_export_simple['raw_original'] = [d['metadata']['raw_nheengatu'] for d in dataset]
    df_export_simple.to_csv(ARQUIVO_SAIDA_CSV, index=False, encoding='utf-8-sig', sep=';')

    # Relat√≥rio Final
    print("\n" + "="*40)
    print("RELAT√ìRIO DE AUMENTA√á√ÉO DE DADOS (V2)")
    print("="*40)
    print(f"Linhas Originais (Excel): {estatisticas['original_rows']}")
    print(f"Linhas Geradas (Expandido): {estatisticas['expanded_rows']}")
    print(f"Fator de Multiplica√ß√£o: {estatisticas['expanded_rows']/estatisticas['original_rows']:.2f}x")
    print(f"Exemplos com [UNK]: {estatisticas['unk_tokens']}")
    print(f"\n‚úÖ Dataset pronto para treino salvo em: {ARQUIVO_SAIDA_JSON}")
    print(f"üìä Tabela para confer√™ncia salva em: {ARQUIVO_SAIDA_CSV}")

if __name__ == "__main__":
    main()